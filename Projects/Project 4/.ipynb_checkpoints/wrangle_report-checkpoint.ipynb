{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcc65d0",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "> Create a 300-600 word written report called wrangle_report.pdf or wrangle_report.html that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a393",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "There are 3 sources of data. The main source is \"The WeRateDogs Twitter archive\" which is data collected from tweets called \"twitter_archive_enhanced\". The second source is located on UDACITY's servers \"The tweet image predictions called \"image_predictions.tsv\". The third source of data is collected via the twitter API, and the dataset is called tweet_json.txt.\n",
    "\n",
    "Starting of by having the datasets *pd_archive*, *pd_prediction* , *pd_tweet_json*, 8 quality issues and more than 2 tidiness issues was identified iteratively by assessing the datasets visually and programmatically. The quality and tidiness issues was solved and the dataset was analysed to provide 3 insights and 1 visualization.\n",
    "\n",
    "This report is divided in three sections; *Gathering Data*, *Assessing Data* and *Cleaning Data*  with a concluding chapter in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d189a",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "The data gathering part consisted of three steps for the three data sources to end up with the files to start assessing:\n",
    "> **WeRateDogs Twitter Archive:** Downloaded from UDACITY website. Here, no initial wrangling was made to create the dataset. The resulting dataframe is pd_archive.\n",
    "\n",
    "> **Tweet Image Predictions:** Downloaded from UDACITY website. Here, the task was to download the file via a request. This is not possible through the Volvo proxy. Instead, this was also downloaded via UDACITY. No initial wrangling was made to create the dataset. The resulting dataframe is pd_predictions.\n",
    "\n",
    "> **Twitter API:** ALSO downloaded from UDACITY website. The plan here was to use an API, which required me to create a twitter account. I choose not to, but included the code from UDACITY in the notebook. The format of this .txt file forced me to create a loop to create a pandas dataframe with the relevant columns (tweet_id, retweet_count and favorite_count). The resulting dataframe is pd_tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061046c4",
   "metadata": {},
   "source": [
    "# Assessing Data\n",
    "\n",
    "The datasets was assessed visually by simply printing out the dataframe, exploring various index ranges to see if i could find anything odd. The datasets were also assessed programmatically by using various functions. The functions were chosen to find deviating patterns or small counts of various categories. \n",
    "\n",
    "In the end i listed all the faults i found and then listed them in their respective assessment quality/tidiness category for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056481e",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "\n",
    "The data cleaning was straight forward coding. I first used the assessment description as a start and implemented the filter. After the filtering was done i asserted the dataframe to validate that the correct datapoints were removed. This was done on a copy of the original dataframe. \n",
    "\n",
    "Some steps were harder than others. The data quality cleaning was simpler and more intuitive than the tidiness issues. The tidiness issues were structural problems within each row, that could be hard to \"fix\", whereas the quality issues were more of a filtering character. \n",
    "\n",
    "The end goal was to end up with a dataframe, for each data source, that had good enough quality to serve as a basis for analysis. The better the dataframe, the more correct analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f51d4",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "There is a reason people in the business often repeat the same quote: \"80 percent of data science is data wrangling\" (which may not be relevant today, though, but you get the point). The quality checks and the iterations a data scientist go through just to end up with a dataset is time consuming, but important. \n",
    "\n",
    "Data wrangling is the most important step when doing data analysis work IMO. It gives a sense of control over the dataset, which in the end only helps the credability of the analysis. If you are able to convey the strength and weaknesses of a dataset, then the analysis will be smooth as butter.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
